% section 2: Components

\section{Components}

Harness

Pipelines

Database

Events Broker

\subsection{Overview of Processing Flow}

As in DC2, processing flow is guided by three production pipelines. Each of
these pipelines will be explained in greater detail below.

\being{itemize}

\item The \textit{image processing and source detection} (IPSD) pipeline takes 
as input the raw exposure images along with the related calibration data and 
produces a catalog of the light sources found within those images.

\item The \textit{Moving Object Processing System} pipeline (MOPS) takes
time and coordinate information from the exposure and creates
a catalog of known solar system objects expected to be within
the exposure FOV.

\item The \textit{association pipeline} (AP) then correlates the sources found
by the IPSD with known objects --- either fixed objects or objects expected
by MOPS to be within the FOV --- to determine whether unexpected sources
have been detected.

\end{itemize}

\subsubsection{Image Processing and Source Detection (IPSD)}

The IPSD pipeline processes images in pairs: two exposures, designated 0 and 1, 
are considered to have been taken consecutively of the same field of view at 
essentially the same time. This pairing of exposures allows for the detection 
of cosmic rays appearing in one exposure but not the other.

In order, the IPSD does the following:

\begin{itemize}

\item Information is read in identifying the exposures to be processed, and 
links are created in the file system from the working directory to the input
images.

\item For image 0 of the pair, metadata in the FITS file header is read in, 
giving the exposure time, location of the field of view, and similar information, 
which is then persisted to the clipboard.

\item Given this information, the calibration products associated with that
exposure are identified by lookup. Calibration products include darks,
flats, bias, and scatter exposures associated with a given camera run.

\item These calibration products are then used to perform instrument
signature removal (ISR) on the raw exposure, resulting in a calibrated image.

\item Source detection is then performed on calibrated image, giving source
information needed later for determining the WCS coordinates of the
exposure.

\item The point spread function (PSF) of the exposure is determined.

\item The second image of the pair is read in and also goes through ISR.

\item WCS of the exposures is determined.

\item The calibrated exposures are then persisted.

\item A template image representing the ideal expected exposure is
created. When processing CHFT-LS images, these templates are 
resampled from stacked images provided by the CHFT-LS survey.

\item Calibrated images 0 and 1 are subtracted from the template
image, and the difference images created are persisted.

\item These difference images are added together. Source detection
and measurement is then performed. The sources detected are
persisted in the database.

\item The association pipeline is signaled through the events broker
that the processing of the image pair is complete and that the source
data is ready for the association process.

\item SDQA data from the exposure is persisted.

\end{itemize}

\subsubsection{Moving Object Processing System (MOPS)}

\subsubsection{Association Pipeline (AP)}

\subsection{Applications}

\subsection{Hardware Deployment}

\subsubsection{The NCSA LSST Cluster}

Most of the DC3a preliminary and production runs were performed on a dedicated 
ten-nodes Dell Server Xeon cluster hosted at NCSA. This heterogenous cluster 
consists of two Xeon 3.6 GHz single dual-core nodes, two Xeon 3.6 Ghz dual
dual-core nodes, and six Xeon 2.0 Ghz dual quad-core nodes. It uses a
gigabit ethernet interface.

This cluster was built for DC2 and the DC2 runs were performed there; for DC3, 
the cluster was upgraded from 32-bit Red Hat Enterprise Linux 4 to 64-bit
Red Hat Enterprise Linux 5. Each cluster node has 4 GB memory, with the exception
of \texttt{lsst10}, which has 16 GB. Each node has from 20 to 60 GB of local disk,
along with 15 TB of disk storage shared among nodes using the NFS shared file system.
As part of DC3a, we installed the Lustre parallel file system to increase I/O 
throughput. We were not, however, able to stabilize the Lustre installation
sufficiently to use it for production runs on the NCSA cluster, as planned.

\subsubsection{NCSA Abe}

For purposes of testing the scalability of the pipelines, additional runs were
dual quad-core Xeon 2.33 GHz processors; for the LSST runs we used 36 of these 
nodes, for a total of 288 cores, sufficient to process an entire focal plane 
for the CHFT-LS images. Each core has 1 GB of memory, and shared access
to 100 TB of disk storage as managed using Lustre.

Abe job runs were coordinated using the Condor-G jobs management system.
The events broker and database were not moved to Abe during these runs;
they remained on the LSST cluster.