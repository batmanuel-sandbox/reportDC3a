% section 3: Software Development Practices

\section{Software Development Practices}

The development practices of the LSST development team were described in detail
in the report on Data Challenge 2. This section restates some central points and
describes some new features of the software development environment.

\subsection{Technical Control Team}

LSST DMS is a distributed development project stretching from Princeton to UC
Davis. This geographical distribution makes it important to have
well-considered, well-documented coding practices.

A goal of DC3 was to increase the formalization of the development process as a
way to compensate for the increasing size of the increasingly distributed
development team. Accomplishing this goal called for a more central role for the
Technical Control Team (TCT), known in DC2 as the Configuration Control Board
(CCB), which serves as an internal forum on key topics for the development of
LSST Data Management software, including major design issues, the tools to be
used, and standards and policies. The TCT meets monthly to set development
policy on matters such as consideration and approval of LSST coding standards,
overall LSST package boundaries (that is, at the broadest level, determining
which component in the LSST stack is responsible for which functionality), and
the adoption of third-party open source software packages for use within the
LSST stack.

\subsection{Open Source Software: Use and Distribution}

The use of third-party open source packages is essential for LSST; these
packages represent standard off-the-shelf solutions for many of those LSST DMS
requirements which are not unique to LSST, freeing developers to concentrate on
those areas of LSST DMS for which no off-the-shelf solution exists. But these
packages must be carefully considered, and they must be agreed upon by the TCT.
In some rare cases, the TCT can withdraw its acceptance of a package. After DC2,
for example, it was judged that the CORAL and SEAL third-party packages
(developed by CERN) should be removed from the LSST stack, primarily because the
difficulty of building these packages from source outside the CERN-approved
Scientific Linux environment in which it was developed forced us to distribute
it as a pre-compiled binary, severely limiting the platforms on which the LSST
DC2 stack could run. The TCT determined that the functionality CORAL and SEAL
provided could be provided more effectively and flexibly with a comparatively
small amount of LSST-created code, thereby releasing the platform constraints.

The LSST stack is itself licensed, by TCT determination, under the GPL3 Gnu
General Public License\footnote{http://dev.lsstcorp.org/trac/wiki/SWLicense}.
GPL3 allows others to copy, redistribute, extend, or modify the software as long
as source code for such extensions is still distributed in source form. It does
not allow the software to be used as part of a proprietary software
package in the absence of second license.   


\subsection{UML Modeling}

UML modeling continues to play a central role in the code development process.
UML is a set of conventions for diagramming abstract models
for object-oriented development. There is a general LSST model meant for the
final production code, and specializations of that model for the specific subset
of that model that falls within the scope of the current data challenge. 

UML design of a component is driven by use cases, the scenarios under which the 
component will be used. As in previous data challenges, the UML models are 
created and maintained using a commercial software package, Enterprise 
Architect\footnote{http://www.sparxsystems.com.au/}, allowing for interactive
and collaborative design. The Enterprise Architect installation is hosted
by LSST in Tucson, but can be accessed remotely using remote desktop viewing
via VNC (Virtual Network Computing) utilities. 

New components or software stages are first designed in UML, and before 
any code is written the model is given a design review. 
At the end of each data challenge, the general LSST model is updated using 
the UML documents created as part of that data challenge; the designer
indicates which aspects of the UML design for the data challenge are intended
to become a permanent part of the general LSST model and which aspects are
for that data challenge alone.


\subsection{Software Development Environment}

One new form of software quality assurance brought into DC3a was the use
of automated testing of code builds. This involved the use of an 
automated ``buildbot'' which on a daily basis builds the software stack 
from scratch, and builds all components from the LSST software trunk against
a standard stack of utilites, producing a report via web 
interface\footnote{http://dev.lsstcorp.org/buildbot/} for displaying the
results of a build.  This display will reveal when a developer checks
in code that ``breaks a package'' (e.g. causing it not to build).  The
LSST buildbot was implemented using the standard open source package
\texttt{buildbot}\footnote{http://buildbot.net/trac} and some
LSST-specific scripts. 

For DC3b, it is anticipated that the buildbot will be expanded to include
automated checking of the package source code to verify compliance with
the associated LSST coding standard. The TCT has investigated several
static analysis tools for C++ standards checking. Such tools can verify
compliance with many of the LSST coding standards, although some of
our standards (such as 3-4 of the C++ standard, which calls for function
names to be verbs) cannot easily be checked by machine and, therefore,
total automated coverage of our coding standards is unlikely.

\subsection{Software Integration Schedule}

For both DC2 and DC3a, the data challenge schedule was punctuated with
a post-development integration period, highlighted by an
``integration week'' workshop in which developers gathered in person
at one institution pull all remaining pieces of the software for
final execution.  While this milestone (which, in terms of travel and
logistics, had to be planned well in advance) was useful for
motivating developers, the software development and debugging work was
still well underway by the time the DC3a integration week began.  This
made it difficult to integrate and test the parts that were complete.  

The seeds of these difficulties were sown early in DC3a in which the
code tree went under significant refactoring and upgrading for 64-bit
support.  While important and necessary, this effectively disabled use
of our code from DC2 until the refactoring was complete.  Thus,
developers were left without the ability to test implementations
inside the harness framework early in the cycle.  This also caused the
delay in the development of effective support and testing tools (which
tend to be driven by demand and experience) later in the cycle.

In DC3b, we plan to employ a number of strategies to avoid the late
integration issues from DC3a in favor of an approach of continuous
integration through out the development phase.  These include:

\begin{itemize}
\item Ensuring the DC3a code stack remains available for testing and
  integration throughout the DC3b development phase.
\item Limit updates to the build environment (which is critical to
  developer progress) to the first two months of DC3b.
\item Identify critical updates to the middleware that must be
  delivered within the first two months of DC3b to allow 
  creation of application early and continuously in the development
  schedule. 
\item Design and develop pipeline stages as semi-functioning stubs
  early in DC3b, prior to full implementation of the underlying
  algorithmic code.  
\end{itemize}

