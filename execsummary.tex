\pagebreak
\section*{Executive Summary}
\addcontentsline{toc}{section}{Executive Summary}

\subsubsection*{Introduction}

Data Challenge 3 (DC3) is the third in a series of prototypes of the
LSST Data Management System (DMS). Through these data challenges, we
seek to prototype and validate proposed solutions to the most 
challenging technical problems to building a
DMS that meets the LSST science goals.

In Data Challenge 1 (DC1), we focused on the DMS middleware design
for supporting nightly processing; this software scaffolding ---
the \textit{pipeline framework} --- modeled the infrastructure
necessary to support science algorithms but did not actually execute
them, using instead \textit{resource consumers}, which simulated the
expected computational node of the algorithms but produced no useable
science output.  In Data Challenge 2 (DC2), we replaced the resource
consumers with real implementations for the most challenging aspects of
the Alert Production, focusing on image processing. We also updated the pipeline
framework as development of the scientific algorithms revealed
new framework requirements.

The scope of DC3 is significantly larger than that of the previous
data challenges, including both a more capable implementation of the
Alert Production and a first prototype implementation of the Data
Release Production.  Because of the large scope of DC3, it is
divided into two phases, DC3a and b.  DC3a, the subject of this report, included only the Alert
Production capabilities, while DC3b adds the Data Release Production.
Other aspects of DC3a included improvements to the application framework
and middleware, along with the following new capabilities: the
Instrument Signature Removal (ISR) pipeline, World Coordinate System
(WCS) determination within the Image Characterization (IC) pipeline,
and an initial SDQA system. Finally, in DC3a significant improvements were 
made in the execution speed of the science applications,  and incremental improvements were 
made in science data quality and within the software build system.

\subsubsection*{Productions Runs Performed and Scalability}

As in DC2, the astronomical images we used as input were from the CFHT-LS
Deep Survey fields D3 and D4.  We also executed some but not all pipeline 
stages using images from the LSST simulated data collections, SimWide and
SimDeep, and these runs were instrumental in understanding and
correcting processing problems.

After initial short runs for purposes of debugging, 
we executed larger-scale DC3a production runs on clusters at
NCSA.  For final analysis of data quality and processing performance,
we have concentrated on two production runs on the NCSA LSST
development cluster; these are referred to by their run
identifiers, \texttt{rlp1233} and \texttt{rlp1234}. These runs used
identical versions of the software and were configured identically, but
each processed a different set of 31 amplifiers in the focal plane for 85
visits to the CFHT-LS D3 field.  To explore the scalability of
the pipelines, we also executed runs of the image processing pipeline
on the NCSA cluster Abe.  These runs were performed across 36 8-core
nodes, processing the entire 288-amplifier focal plane for CFHT-LS
simultaneously.  (Some of the supporting software services for these runs, such as
the event broker and the database, remained on the NCSA LSST cluster.)

These runs showed that the pipelines scaled reasonably well to the
level of 10\% of the actual LSST image size, although running at this scale did reveal some additional
configuration requirements for the event broker and the MySQL
database.  To run on Abe, we integrated grid-based technologies
--- including grid-based job management using Condor-G --- into our the
pipeline orchestration layer.  While short of our goal of 15\% of the actual LSST image size,
we elected to close out DC3a at this level in order to expedite completing this report.
We expect to complete runs up to the 15\% level before PDR.

\begin{table}[ht]
\centering
\caption{Summary of Production Runs analyzed in this report.  The
output image size includes all intermediate products, the majority of
which will not be persisted by the LSST.
\label{ex:tbl:runsummary}}
\vspace{\baselineskip}
\begin{tabular}{rccccc}
\hline\hline
          &         &       & \multicolumn{3}{c}{Images Processed: Num/Size} \\
runId     & nVisits & nAmps & Raw Images & Ancil. Images & Output Images \\ \hline
\texttt{rlp1233}   & 85      & 31  & 5146/24 GB    & 88/13 GB       & 60525/240 GB \\ 
\texttt{rlp1234}   & 85      & 31  & 5146/24 GB    & 88/13 GB       & 60525/240 GB \\ 
\texttt{rlpabe041} & 12      & 288 & 3456/33 GB    & 15/2 GB        & 79368/313 GB \\ \hline
Total     & ---     & 250 & 13748/81 GB   & 191/28 GB      & 2000418/793 GB \\ \hline
\end{tabular}

\end{table}

Finally, in DCs 1, 2, and 3a, the availability of computational and storage resources has not
significantly impacted the scaling tests possible.  Since the Data Release Production will 
involve significantly larger data volumes and computational load, the volume
and performance achievable in DC3b will be limited by the available resources.

\subsubsection*{Algorithm/Application Performance}

While the total processing time per
visit is longer than in DC2, we note that DC3a does significantly more processing,
including additional caching of intermediate results that would not be
necessary in production mode.  When we compared the common stages of
processing in DC2 against their counterparts in DC3a, we saw large improvements across the
board, along with a much lower variation in processing times over
different data.  The most expensive stage, image subtraction, saw a
2-fold improvement over DC2; subsequent experiments showed tuning
configuration parameters increased that factor to 3.

While the improvements are significant and on a pace 
with our expectations for this stage of the project, 
we are still not meeting the full latency and throughput
requirements of the operational DMS by approximately a factor of 4.  As we do not expect 
this gap to be closed significantly by basic improvements in cluster hardware, 
we are targeting both algorithmic
improvements and alternative hardware architectures (GPUs) in future DCs.

\begin{table}[ht]
\begin{center}
\caption{Average Processing Times Per Visit
\label{ex:tbl:visitstats}}
\vspace{\baselineskip}
\begin{tabular}{ l | c | c }
\hline\hline
          & Total Processing Time, corrected
          & Application Processing Time \\ 
Pipeline  & average $\pm$  $3\sigma$ (s) & average $\pm$ $3\sigma$ (s) \\ \hline
IPSD      & 264.7 $\pm$ 20.2 & 264.1 $\pm$ 20.1  \\ 
nightmops & 10.6  $\pm$  4.4 & 10.4 $\pm$  4.4  \\ 
ap        & 1.4   $\pm$  0.7 & 1.2 $\pm$  0.7  \\ \hline
\hline
\end{tabular}

\end{center}
\end{table}

Finally, the results continue to show that the overhead 
added by the  pipeline framework is minimal.
The comparison of the runs also revealed technology-, platform-, and configuration-specific results for stages
handling I/O, particularly when we considered filesystem and database
I/O separately. In DC3b, we will spend more effort developing better I/O
strategies, optimizing the use of parallel filesystems and deploying distributed databases.  

\subsubsection*{Science Data Quality}
As in DC1 and DC2, DC3a did not focus on major improvement to science data quality.  
In DC3b this will be a major focus, and we anticipate significant involvement from
members of the LSST Science Collaborations in evaluating data quality and algorithm accuracy
and efficiency.

Nevertheless, we employed both algorithm developers and independent
 (non-DC) scientists from IPAC
to evaluate the science data quality of DC3a in the areas that are
judged to be the most demanding for the software system to meet, 
and are each directly related to science requirements:
\begin{itemize}
\item WCS accuracy was acceptable in 98\% of images processed, with median errors of 40 milli-arcsec.
\item Expressed as a ratio to the irreducible image noise, the noise level away from bright stars in difference images with acceptable WCS is near 1.0, as expected.
\item The noise level ratio near bright stars in difference images with acceptable WCS is near 20 (unacceptable), and this will be addressed in DC3b by refining the kernel basis functions chosen for image subtraction.
\item Difference image lightcurve quality was adversely affected by residuals around bright stars and will be re-evaluated once the basis functions are addressed.
\end{itemize}

\subsubsection*{Project Management}
A comparison between DC2 and DC3a reveals the following productivity statistics:
\begin{itemize}
\item 28 staff at 30\% time average in DC3a vs.~19 staff at 50\% time average in DC2;
\item 9 months estimated/14 months elapsed time in DC3a vs.~10 months estimated/12 months elapsed time in DC2;
\item 107k lines of code produced (c++, python, SQL, java, scripts) in DC3a vs.~90k in DC2.
\end{itemize}

As can be seen from the above, in DC3a there was  a significant schedule overrun (50\%).
The team was very late in establishing the final scope 
and developing resource-leveled plans. The reasons for this include a larger team but
 lower per-person involvement than in DC2.
This caused more and longer periods where parts of the team were blocked
waiting for other parts to complete, as well as requiring more time to be spent 
on coordination between locations.
In addition, some major items were significantly underestimated (e.g. reworking of the core image classes
in the Application Framework) and should have been better specified early on.
Changes are being made to the team organization and process to improve this in future DCs.
