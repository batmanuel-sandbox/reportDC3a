% section 5.1.2: Pipeline Execution

\subsubsection{Pipeline Execution} \label{sec:pex}

Pipeline execution is handled by the {\tt pex\_} family of packages.
Utility functionality is provided via {\tt pex\_exceptions,
pex\_policy,} and {\tt pex\_logging}.  The core of pipeline execution
is handled by {\tt pex\_harness} which provides the realization of the
{\it pipeline harness}, summarized in section \ref{sec:components}.

\subsubsubsection{Pipeline Execution Utilities}

The pipeline execution utility packages underwent various
improvements in DC3a.

\paragraph{Exceptions.}  The exception framework was overhauled in
part to make it easier to create, throw, and handle exceptions.  In
particular, support was improved for a type of traceback that can preserve
context-specific error messages as an error condition travels upward
through a call stack: as an exception is caught and rethrown, handlers
can add more information that better describes the context that
resulted in the error.  We also changed the way exceptions travel
across the C++/Python barrier: C++ exceptions are converted to the
{\tt LsstCppException} class in Python, which inherits from the
standard Python {\tt Exception} class.  This has the advantage
allowing all exceptions to be caught via a single {\tt Exception}
class; however, this is at the cost of easy, highly customized
handling of exceptions based on type.  We are still looking for a
strategy enabling both highly desirable features simultaneously.  

\paragraph{Logging.}  In DC2, there existed two types of logging:  one
primarily for debugging messages that would normally be automatically
turned off during production runs, and one for messages that should be
persisted.  In practice, there has not been a clear distinction
between the two types.  For DC3a, we unified both types into a single
unified logging framework while preserving the APIs established under
DC2.  

\paragraph{Policy.}  The {\tt Policy} class, which is used to access
the data stored policy files used to configure pipelines, was updated
to use the new {\tt PropertySet} internally.  The {\tt Policy} API was
altered as well for greater consistency with the {\tt PropertySet}
API.  

\subsubsubsection{The Pipeline Harness} \label{sec:harness}

At the core of {\tt pex\_harness} is the {\tt Pipeline} class, which
drives the master process of a pipeline and executes the serial portion of
each stage.  It also spawns the worker processes --- the {\it slices} --- on
all of the nodes hosting the pipeline.  The {\tt Slice} class drives
the processing in each worker process, taking its direction from the
remote {\tt Pipeline} via MPI messages.  Both the {\tt Pipeline} and
the {\tt Slice} workers execute a loop of application stages as
configured by pipeline policy.  A primary driver for the construction
of long lived parallel {\tt Slice} worker threads is the ability to
hold image and other data in memory as the processing advances from
one module to the next. The in-memory data structure that holds the
collection of data on which applications operate is defined as a {\tt
Clipboard}.

The software container that the harness provides for hosting an
algorithm written by an application developer is the {\tt Stage}
class.  The {\tt Stage} class provides a standard API for integrating
code into the LSST pipeline harness framework.  The pipeline harness
links the modular application stages to standard interfaces for I/O
functionality and access to the persistence layer, abstracting this
functionality away from the application code.

Important improvements made to the pipeline harness for DC3a included:

\begin{itemize}
\item The harness middleware package was refactored to operate with
the upgraded software framework classes, i.e., modified to use 
{\tt PropertySet}, new persistence formatters and logical location setting,
new {\tt Exception} classes, etc.
\item Infrastructure required for inter-slice communication using
underlying {\tt boost::mpi} libraries (with boost serialization) was
developed within pipeline middleware.  Inter-slice communication
allows complex types collected in a {\tt PropertySet} to be
communicated via MPI, effectively transferred from the {\tt Clipboard}
of one {\tt Slice} to that of another.  This functionality was not
utilized within astronomical algorithms in DC3a, but the
groundwork laid within DC3a will allow for more rapid inclusion into
suitable algorithms for DC3b and the future.
\item The harness logging was upgraded with the ability to write log
messages to local log files, with a separate file for each {\tt Pipeline}
and each {\tt Slice} worker.  This functionality greatly facilitates
debugging as middleware code and application stages are developed and
tested.  In addition, a new logging pattern was employed for
instrumenting pipeline performance that makes it easier to extract
execution times via an event monitor script (see
section \ref{sec:evmon}).  
\item Pipeline policy handling was updated to the {\tt Policy} class's
built-in facility for policy file inclusion.  Besides providing
greater consistency with normal policy usage, this change was
important for facilitating provenance recording.  
\item A {\tt SimpleStageTester} class was added to provide a
light-weight container for testing a single {\tt Stage} implementation
in a single process (non-MPI) context.  This class proved quite
effective for creating small scripts that replicated known bugs.  
\end{itemize}

