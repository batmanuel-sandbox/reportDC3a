% section 4: Input Data

\section{Input Data}


\subsection{CFHT-LS Deep Survey Data}

32 CCDs.

\subsubsection{Science Data Products}

The ``raw'' archival science data obtained from CFHT have  gone through
a modicum of pre--processing by the CFHT {\tt ELIXIR}
pipeline\footnote{\url{http://cfht.hawaii.edu/Instruments/Imaging/MegaPrime/rawdata.html}}.
In particular, the two amplifier readouts were spliced back together
into 2112 by 4644 pixel images corresponding to a single CCD.  To prepare the data for DC3a processing, we need
to undo this operation to some degree, while still maintaining
consistency between the valid data sections, overscan regions, and
approximate Wcs information in the image headers.

For DC3a processing, we divided each of the 32 CCD images into 8 images
that are approximately the anticipated size of LSST amplifier images.
Synthesized amplifier images 0-3 come from CFHT image subsection {\tt
[0:1056,0:4612]} of the input images, and amplifier images 4-7 from
{\tt [1056:2112,0:4612]} (there is an additional overscan region from
$4612 \leq y \leq 4644$ that is ignored).  Each resulting amplifier
image is 1056 x 1153 pixels, with 32 pixels of overscan implemented as
``prescan'' (amplifiers 0-3) or ``postscan'' (amplifiers 4-7).  When segmenting the images, we adjust the following Metadata keywords :

% NOTE - I coped these sections straight out of 
% prepdc3a/python/stageCfhtForDc3a.py.  Unfortunately
% these data sections end up being a mix of python slicing 
% (0-indexed) and IRAF/FITS convention (1-indexed).  How did I 
% deal with the fact that the sections are originally defined 
% in this convention?  This is dealt with in Isr.cc, BBoxFromDatasec.

\RHL{Do we need this much detail?}

\begin{itemize}

\item {\tt RDNOISE} : Each amplifier has a different amount of
readnoise.  In the spliced CFHT CCD image, these are recorded as {\tt
RDNOISEA} and {\tt RDNOISEB}.  For our segmented amplifier images we
assign {\tt RDNOISE} as either {\tt RDNOISEA} (amplifiers 0-3) or {\tt
RDNOISEB} (amplifiers 4-7).

\item {\tt GAIN} : Derived from {\tt GAINA} and {\tt GAINB} in a
manner similar to the {\tt RDNOISE} field.

\item {\tt BIASSEC} : This is set to either columns {\tt [1:32,]} (amps
0-3) or {\tt [1025:1056,]} (amps 4-7).

\item {\tt DATASEC} and {\tt TRIMSEC} : This is set to either columns
{\tt [33:1056,]} (amps 0-3) or {\tt [1:1024,]} (amps 4-7).

\item {\tt CRPIX1} : This is left as--is for amps 0-3, and corrected
by -1023 pixels for amps 4--7.  Additionally, for CCDs 1--16, amps
0--3 are adjusted by another -33 pixels, and for CCDs 17--32 amps 4--7
are adjusted by another -33 pixels.  This reflects the exclusion of a
secondary overscan region at the ``top'' or ``bottom'' of the CCD,
depending on the orientation of the CCD on the focal plane.

\item {\tt CRPIX2} : This is adjusted by the y--axis offset of each
amplifier image from the original CCD image.

\end{itemize}

The visitId associated with each Image comes from the Metadata keyword
{\tt OBSID}, and we assume that this image represents the first
exposure (exposureId = 0) of the cosmic ray split.  The second exposure of the visit (exposureId = 1) 
is synthesized by taking the actual
CFHT image and adding a set of cosmic rays.  The segmented
files are saved on disk as Images using the following formatting :
{\tt 'raw-\%06d-e\%03d-c\%03d-a\%03d.fits' \% (visitId, exposureId,
ccdId, ampId)}

\subsubsection{Calibration Data Products}

The CFHT calibration data includes FITS keywords that define the range
of dates for which it is valid.  In order to process CFHT science frames
we needed to associate a given filter/date with the proper calibrations.  We
accordingly generated a \texttt{.paf} (i.e. \texttt{Policy}) file that may
be read into a \texttt{Policy} with nested fields that enable the fast
lookup of the correct calibration file.

According to Astier, ``the Elixir flats that you download from CADC
are not fully correct on large scales. Namely, the flux of the same
star still varies by a few percent center-to-corner'';  we have not
attempted to correct for this effect in DC3a,  although it is clear
that the final LSST system will need to do so.

\subsection{LSST Simulated Images and Catalogs}

Three sets of simulated images were generated for the LSST data
management pipeline: ``Deep'' images comprise 96 CCD images of the
same field (with varying seeing, airmass and sky backgrounds) and were
designed for use in image stacks, ``Wide'' images comprise 163 CCDS
which sample 3 rafts from the center to the edge of the focal plane
over a range of airmasses and seeing conditions, ``Ideal'' images
comprise a simulation with no atmosphere, no optical distortions,
airmass of unity, 0.1 arcsec seeing and no sky background (this
defines an idealized view of the sky for use as a truth table).

The underlying stellar and galaxy catalogs are based on the
simulations of \cite{Juric+Ivesic} and \cite{kitzbischler}
respectively.  To reproduce the LSST sky coverage the 2x2 deg galaxy
catalogs from Kitzbichler \& White are tiled across the sky.  The SED
of each galaxy is estimated using the Bruzual \& Charlot (2003) stellar
population models. Using the models of Batissta et al (2006) each
galaxy in the Millennium Simulation is assigned a morphological
profile. The current implementation assigns each galaxy a
disk-to-total flux ratio, position angle in the sky, inclination along
the line-of-sight, bulge radius, and disk radius.

Stars are specified by SED, and positions. The SEDs for stars are
derived from Kurucz models (i.e.\ excluding dwarf and giant
stars). The model used to generate main sequence stars is based on
work done by Juric et. al. This includes user-specified amounts of
thick-disk, thin-disk, and halo stars. Each version of a catalog
contains metadata on metallicity, temperature, luminosity-type, and
surface gravity.

From these data, images with varying PSF (from 0.4 to 1.8 arcseconds),
airmass (from 1.0 to 2.0), sky background ( with low, new moon, and
high, full moon and 20 degrees from the Moon), and varying mirror and
optics specifications (including aberrations) were generated.  The
limiting magnitude of these catalogs was $r<26$ and the catalogs were
made available with the resulting images.

\subsubsection{Science Data Products}

Images were output in standard FITS format. Each 65MB FITS file
represents a single 4096x4096 CCD 15 second exposure with 0.2 arcsec
pixels and covers 13.6x13.6 arcminutes of the sky. Each FITS file
contains keywords required by the LSST data reduction pipelines
including date, data sections, saturation level, and world coordinate
system (WCS).  Amplifier based images were constructed by breaking up
each CCD into 16 amplifier images (512 $\times$ 2045 pixels), addition
of an overscan region of 5 pixels, and inclusion of read noise of 5
elections.  A software bias of 100 counts was added to all the pixels
to avoid negative counts.  All amplifier-level FITS images were cast
as 16-bit unsigned integer. The FITS keywords for these images include
the simulation run name (i.e. Wide, Deep, or Ideal), dates processed
at University of Washington and generated at Purdue, along with CCD
processing information (i.e. DATASEC, etc..)

The Image Simulation results predict non-linear distortions in the
coordinate system, at the sub-pixel level, across a CCD. To meet the
LSST science-goal astrometric requirements (15 milliarcsec) and to
account for these non-linear optical distrotions, a TAN-SIP WCS
solution was implemented for these images. TAN-SIP provides a
polynomial relation between CCD positions and the position on the sky
(where the order of the polynomial is user-specified). The TAN-SIP
convention is supported by the publically available software package
WCSTools and by Saoimage DS9. 

\subsubsection{Calibration Data Products}

Calibration data products for the simulated images include flat field
images, dark frames, and bias frames. The flat field images were set
to a mean value of unity and the dark and bias frames had a mean value
of zero.

\subsection{Event Generation From Input Data}

The input data exist on disk as Fits files, with all requisite
metadata in their headers.  Conversely, the paradigm for LSST is to
have the pixel--level science data and associated Metadata as separate
entities, the latter ideally existing in a database but in any case
being received separately from the camera system.  Therefore to
generate an event for DC3a processing, we must first address the
images on disk, translate their Fits metadata fields (e.g. 'GAIN')
into LSST--format metadata fields (e.g. 'gain'), assemble selected Metadata
representative of what the camera would know into a PropertySet, and
send this as Event to the pipeline.

This process required a metadata $\rightarrow$ Metadata mapping for
each dataset.  This was implemented as so.
