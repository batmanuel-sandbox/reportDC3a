% section 5.1.3: Pipeline Orchestration

\subsubsection{Processing Orchestration and Control}

Closely related to pipeline execution packages are the control
packages ({\tt ctrl\_}).  The {\tt ctrl\_events} package provides the
event framework that allows pipelines to talk to each other.  The {\tt
ctrl\_evmon} provides the event monitor services which can listen to
events (particularly logging events) and react intelligently.  The
{\tt ctrl\_orca} realizes the {\it orchestration layer} responsible
for launching pipelines.  An important job this layer must do is
record {\it provenance} information--all of the data describing the
software environment and policy paramters that was used to configure
and execute a pipeline.  

\subsubsubsection{Pipeline Orchestration} 
\label{sec:PipelineOrchestration}

In DC2, the orchestration layer was largely undeveloped; in lieu, we
had a customized launching script that handled several of chores that
in general would be handled by the orchestration layer.  In DC3a, we
used this script as a starting use case to design a more generalized
layer configuring and launching pipelines.  The design also included
the notion of monitoring a pipeline as it runs, but we did not
implement that part in DC3a.  

One of the challenges we want the layer to address is adapting a
pipeline to run on different platforms.  These platforms may differ in
terms of, say, where data can be read in from and written out to, how
the pipeline must be launched--e.g. right away via a remote shell
command (as with our development cluster) versus asynchronously via a
batch scheduler (required for NCSA Abe), and how one checks on its
progress.  We also want it to be able to launch any kind of
pipeline--not just one based on our pipeline harness.  To address
these challenges, the {\tt orca} design segregates its various duties
into separable classes, allowing for specialized versions to be
plugged in to handle different types of pipelines and platforms.
Similar, we had to develop a policy configuration that could mirror
the class model and allow, for instance, the specialized classes to be
specified in policy.  

The main duties of the orchestration layer are:
\begin{itemize}
\item Create working directories on the target platforms for input and
  output data, policy files, and log files,
\item Initialize the database tables to be used by the pipelines, 
\item Deploy all needed policy files and launch scripts onto the
  target platforms,
\item Set the necessary software environment, 
\item Record provenance data
\item Remotely execute each platform launch script.
\item Monitor the pipeline for failures (not implemented in DC3a)
\item Shut down the pipeline (not implemented in DC3a)
\end{itemize}

The main classes responsible for this are:

\begin{description}
\item[\tt ProductionRunManager:]  a class for configuring and launching the
set of pipelines that make up a production run.  
\item[\tt PipelineManager:]  a class for configuring and launching a
specific pipeline.  We had two implementations of this abstract class:
{\tt SimplePipelineManager} for launching on our development cluster,
and {\tt AbePipelineManager} for launching on the NCSA Abe machine.  
\item[\tt DatabaseConfigurator:]  a class use by a {\tt
ProductionRunManager} and/or a {\tt PipelineManager} (depending on the
policy configuration) to set up the database tables required by the
pipelines.  
\item[\tt Provenance:]  used by the {\tt ProductionRunManager} to
record the provenance data--namely, the policy data--into the
database.  
\end{description}

It's worth noting that when launching a pipeline, our {\tt
PipelineManager} implementations ultimately end up launching a
pipeline launch script through a remote shell.   This launch script is
one that could be run directly by a user (say, for debugging purposes)
independently from the orchestration layer.  In other words, a
pipeline has no dependencies on the orchestration layer to run; this
is a useful feature of the orchestration layer design.  A consequence
of this is that the policy that will configure the pipeline must be
fully specified, and this must include parameters that would be more
conveniently set at the production level.  An example is the {\tt
eventBrokerHost} parameter which sets the host where the event broker
is running; since the broker is responsible for routing event messages
between pipelines, all pipelines need to use the same broker.  Thus,
to ensure consistency, we want to specify it once for the whole
production, but it is used by each pipeline, and so the pipeline
policy needs to have this information.  

We address this dilemma by allowing the parameter to be set at both
the production level {\it and} the pipeline level.  The latter will be
considered the default value that will get used when a pipeline is
launched on its own, independently from the orchestration layer.
However, when the pipeline is launched from orchestration, the
production level value will override the pipeline level value.  The
way this is accomplished is that before the orchestration layer
deploys the pipeline policy files onto the (remote) pipeline platform,
it collects all of the share policy data and rewrites the pipeline
level policy, overriding the defaults.  This fact has important
implications for provenance.  


\subsubsubsection{Provenance}  \label{sec:provenance}

The orchestration layer (described above) in DC3a generates enhanced
provenance information.  In particular, the software environment and
the contents of the policy files used to run the production are
written both to log files and to the database.  Recording the software
environment allows the exact configuration of LSST-packaged software
used for the run to be reproduced later, while recording the policy
files captures both platform configuration information such as the
compute nodes and database used as well as all configurable science
algorithm settings.

This provenance information, in combination with an event sent to the
production, is sufficient to enable accurate reconstruction of a given
data product resulting from that event, although a demonstration of
automated reconstruction was deferred.  When combined with the full
sequence of events sent to the production, the provenance allows exact
duplication of a given run.  The recorded provenance proved highly
useful while debugging algorithmic issues since it simplified the
construction of small reproducible test cases demonstrating problems.

The software environment is characterized by the versions of packages
maintained by {\tt eups} that are ``setup'' at the time of production
execution (see DC2 report for more details).  In addition, the actual
directories declared as the installation locations of the packages are
also persisted, allowing locally-setup packages and packages installed
under {\tt \$LSST\_DEVEL} to be identified.  We note that in DC3a, the
recording of the EUPS environment happens within the {\tt orca.py}
script on the launch platform.  Consequently, this will only
guaranteed to be accurate if the software stack installed on the
launch platform is identical to the one on the pipeline platform(s);
in general, this is not a good assumption.  The best time to record
this information is just prior to when the pipeline process actually
starts on the remote pipeline platform; this is what will happen in
DC3b.  

The recorded policy file information includes the contents on a per-key
basis as well as an MD5 hash of the file contents and the file's
last-modified-time.  These latter two items are intended for eventual
use to remove duplicate entries when policy files are reused across
multiple runs.

The provenance written to the database goes into two sets of tables: one
set in the per-run database and one in a global database ({\tt
DC3a\_DB}) that spans all DC3a runs.  The global database permits
queries to find runs that used a given configuration.  For example, this
query finds all runs that had the {\tt pixelScaleRangeFactor} set to a
number other than 1.1:

\begin{verbatim}
SELECT prv_Run.runId, prv_PolicyKey.keyName, prv_cnf_PolicyKey.value
FROM prv_Run, prv_PolicyKey, prv_cnf_PolicyKey
WHERE prv_PolicyKey.policyKeyId = prv_cnf_PolicyKey.policyKeyId
  AND keyName = 'pixelScaleRangeFactor'
  AND value != '1.1'
  AND FLOOR(prv_PolicyKey.policyFileId / 65536) = prv_Run.offset;
\end{verbatim}

Similarly, this query finds all runs that used version 3.0.9 of the {\it
meas\_algorithms} package:

\begin{verbatim}
SELECT runId
FROM prv_SoftwarePackage NATURAL JOIN prv_cnf_SoftwarePackage
     JOIN prv_Run ON (FLOOR(prv_SoftwarePackage.packageId / 65536) = offset)
WHERE packageName = 'meas_algorithms' AND version = '3.0.9';
\end{verbatim}

We note that, like the recording of the EUPS environment, the
recording of the policy data is done within the {\tt orca.py} process
on launch platform, not on the pipeline platforms where they are
actually used.  And like the recording of the EUPS environment, there
is a danger that the recorded policy data will not be accurate,
particularly considering the policy rewriting that the orchestration
layer does described in the previous section.  In DC3a, we do
know that the recorded policy data was, in fact, accurate; however,
because of how functionality is encapsulated in the orca classes, the
{\tt Provenance} class cannot make any assumptions regarding how the
policy files were transferred to the pipeline platforms.  Thus, like
the EUPS data, the best time to record the policy data is right before
the pipeline executes; this is how it will be done in DC3b.  

\subsubsubsection{The Event System} \label{sec:events}

The event system described in the DC2 report (section 4.6.1) is
largely unchanged for DC3a (now in {\tt ctrl\_events}):  employing a
publish-subscribe model, event messages sent through the event publish
API are serialized and distributed to subscribers via a central event
broker process (realized using the third-party package, ActiveMQ).
The main change is that in DC3a, we did not use the third-party
package, Mule, to record logging events into the database; this was
instead handled by the new event monitor (next section).  We may still
use Mule as an event router for higher-level DM operations.   

\label{brokerprob} In DC3a, we did notice occasions when the
broker would begin failing when data trigger events were published
faster than they were consumed by the pipelines:  the more events that
were built up in the queue, waiting to be consumed, the more likely
the broker would fail and stop sending events.  Given that the broker
is a completely third-party (though open source) package, the cause of
this is not fully understood.  We were able to work around this by
throttling down the frequency at which data trigger events were
published to the approximate average time to processing a visit (about
4.5 minutes).  

Finally, we also discovered that the broker is subject to the
operating system limit on the number simultaneously open file
descriptors.  This limit can easily be exceeded when a production
includes about 240 or more processes.  Discovered during large runs on
the Abe cluster (discussed in section \ref{sec:recsummary}), this
limit can be increased via an OS configuration change (requiring root
privileges).  

\subsubsubsection{Event Monitoring} \label{sec:evmon}

The Event Monitor (package {\tt ctrl\_evmon}) is general purpose
facility for listening and reacting to events and is new in DC3a.  An
{\it event monitor} is a process that is driven by an event monitor
script which is set up to listen for a specific series of events with
specified attributes.  When those events have been detected (in order
and within a prescribed amount of time), the script will do
something--typically, issue another event.  

The motivating use case for the event monitor is to asynchronously and
remotely detect system faults--in particular, catastrophic failures of
a pipeline.  An event monitor script can be constructed to look for
events from a pipeline that should appear at expected intervals.  If
monitor fails to detect those events, it can issue an event that
announces a failure of the pipeline.

In DC3a, event monitors were used in two ways.  First is as a service
that records logging events into a logging database in real time.  The
second is to calculate the difference in time stamps between specially
constructed log messages that mark out the execution of specific
blocks of code.  These calculations were loaded into the database and
served as the basic data for the timing analysis described in
section \ref{sec:timing}.  This latter use can be done in real-time;
however, in the practice of DC3a, the event monitor script was run
after the production was complete on logging events streamed from the
database.  

